{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM tuning time steps\n",
    "\n",
    "In this notebook we tune the time step used for optimal prediction of the stock prices.\n",
    "We build on the findings in `3.2.5-lstm_prediction_tuning_second_cells.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "from data.get_50_highest_weights import get_sp_50_highest_weights_symbols\n",
    "from data_preparation.ochlva_data import OCHLVAData\n",
    "from utils.column_modifiers import target_generator\n",
    "from utils.column_modifiers import keep_columns\n",
    "from utils.scorers import normalized_root_mean_square_error\n",
    "from utils.transformations import StockMinMax\n",
    "from estimators import lstm\n",
    "from estimators.predictions import calculate_normal_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('nbAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.visualizations import plot_scores\n",
    "from utils.visualizations import plot_true_and_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the S&P 500 (as `^GSPC`) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ochlva_data = OCHLVAData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load three other stocks: The stock weighted the most, the medium weighted stock and the lowest weighted stock (out of the 50 downloaded). \n",
    "We do this in order to get a better feeling of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = get_sp_50_highest_weights_symbols()\n",
    "\n",
    "# Select symbols with high, medium and low weights\n",
    "selected_symbols = (symbols.iloc[0], \n",
    "                    symbols.iloc[len(symbols)//2], \n",
    "                    symbols.iloc[-1])\n",
    "\n",
    "for s in selected_symbols:\n",
    "    ochlva_data.load_data(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will only be interested in training using the adjusted close values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 'Adj. Close' column\n",
    "ochlva_data.transform(keep_columns, ['Adj. Close'], copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the target values for the data.\n",
    "The target columns will be shifted 7, 14 and 28 days with respect to 'Adj. Close'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [7, 14, 28]\n",
    "ochlva_data.transform(target_generator, 'Adj. Close', days, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the time steps on the validation set\n",
    "\n",
    "In order not to leak information of the unseen data into the tuning we will tune the number of the second layer of time_steps on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_epochs = 160\n",
    "optimal_batch_size = 128\n",
    "optimal_drop_out = 0.0\n",
    "optimal_time_steps = 128\n",
    "optimal_cells = 128\n",
    "time_steps_list = [1, 2, 4, 8, 16]\n",
    "\n",
    "validation_scores = dict()\n",
    "train_scores = dict()\n",
    "\n",
    "for key in ochlva_data.transformed_data.keys():\n",
    "    print(f'Processing {key}')\n",
    "    # Extract the features and targets\n",
    "    x = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[:-len(days)]] \n",
    "    y = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[-len(days):]]\n",
    "       \n",
    "    # Append the stock to the scores\n",
    "    validation_scores[key] = dict()\n",
    "    train_scores[key] = dict()\n",
    "    \n",
    "    for time_steps in time_steps_list:        \n",
    "        reg = lstm.LSTMRegressor(time_steps=time_steps,\n",
    "                                 cells=(optimal_cells,), \n",
    "                                 drop_out=optimal_drop_out,\n",
    "                                 recurrent_drop_out=optimal_drop_out,\n",
    "                                 batch_size=optimal_batch_size,\n",
    "                                 epochs=optimal_epochs)\n",
    "        \n",
    "        x_train, _, y_train, y_test = \\\n",
    "            train_test_split(x, y, shuffle=False, test_size=.2)    \n",
    "        x_train_for_validate, x_validate, y_train_for_validate, y_validate = \\\n",
    "            train_test_split(x_train, y_train, shuffle=False, test_size=.2)\n",
    "\n",
    "        # Obtain the day of prediction\n",
    "        # I.e. for a column named x + 2 days, we would expect the two last rows\n",
    "        # to contain nan\n",
    "        prediction_days = y_test.isnull().sum()\n",
    "\n",
    "        # Scale the features\n",
    "        scaler = StockMinMax()\n",
    "        scaler.fit(x_train)\n",
    "        x_train_for_validate = scaler.transform(x_train_for_validate)\n",
    "        x_validate = scaler.transform(x_validate)\n",
    "        y_train_for_validate = scaler.transform(y_train_for_validate)\n",
    "        y_validate = scaler.transform(y_validate)  \n",
    "        \n",
    "        # Calculate validation scores (note that the neural network \n",
    "        # supports multi prediction \"out of the box\")\n",
    "        y_pred, y_pred_train = \\\n",
    "            calculate_normal_prediction(reg,\n",
    "                                        x_train_for_validate,\n",
    "                                        x_validate,\n",
    "                                        y_train_for_validate,\n",
    "                                        y_validate, \n",
    "                                        prediction_days,\n",
    "                                        training_prediction=True,\n",
    "                                        use_multi_output_regressor=False)\n",
    "            \n",
    "        # Restore the scaling\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_pred_train = scaler.inverse_transform(y_pred_train) \n",
    "        y_validate = scaler.inverse_transform(y_validate)\n",
    "            \n",
    "        validation_scores[key][time_steps] = \\\n",
    "            normalized_root_mean_square_error(y_validate, y_pred)\n",
    "        \n",
    "        # The true value of the trainings is the same as y_validate shifted by \n",
    "        # one day\n",
    "        y_train_true = y_train.loc[y_pred_train.index, :] \n",
    "        train_scores[key][time_steps] = \\\n",
    "            normalized_root_mean_square_error(y_train_true, y_pred_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_scores(train_scores, validation_scores, x_label='Time steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that a time step of 1 is optimal, and that we are overfitting for higher time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the unseen test set\n",
    "\n",
    "We will now test how well the model with the features generalizes on the unseen test set.\n",
    "This will also act as a sanity check in order to see that what we have found so far is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimal_time_steps = 1\n",
    "\n",
    "for key in ochlva_data.transformed_data.keys():\n",
    "    \n",
    "    x = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[:-len(days)]] \n",
    "    y = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[-len(days):]]\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                        shuffle=False, test_size=.2)\n",
    "    \n",
    "    # Obtain the day of prediction\n",
    "    # I.e. for a column named x + 2 days, we would expect the two last rows\n",
    "    # to contain nan\n",
    "    prediction_days = y_test.isnull().sum()\n",
    "    \n",
    "    # Make the regressor with the optimal n\n",
    "    reg = lstm.LSTMRegressor(time_steps=optimal_time_steps,\n",
    "                             cells=(optimal_cells,),\n",
    "                             drop_out=optimal_drop_out,\n",
    "                             recurrent_drop_out=optimal_drop_out,\n",
    "                             batch_size=optimal_batch_size,\n",
    "                             epochs=optimal_epochs)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StockMinMax()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = scaler.transform(y_train)\n",
    "    y_test = scaler.transform(y_test)  \n",
    "    \n",
    "    # NOTE: We refit the model here with the same architecture as we used in the \n",
    "    #       predictions above\n",
    "    y_pred = calculate_normal_prediction(reg,\n",
    "                                         x_train,\n",
    "                                         x_test,\n",
    "                                         y_train,\n",
    "                                         y_test, \n",
    "                                         prediction_days,\n",
    "                                         use_multi_output_regressor=False)\n",
    "\n",
    "    # Restore the scaling\n",
    "    x_test = scaler.inverse_transform(x_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_test = scaler.inverse_transform(y_test) \n",
    "\n",
    "    # Plot the results\n",
    "    _ = plot_true_and_prediction(x_test, y_pred, \n",
    "                                 columns=['Adj. Close'], y_label='USD')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate the normalized root mean squared error\n",
    "    nrmse = normalized_root_mean_square_error(y_test, y_pred)\n",
    "    \n",
    "    print((f'Normalized root mean squared error (averaged for the three '\n",
    "           f'predictions): {nrmse}'))\n",
    "    \n",
    "    print('-'*80)\n",
    "    print('\\n'*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
