{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data analysis\n",
    "\n",
    "In this notebook we will perform some simple analysis of the data.\n",
    "We will look at some descriptive statistics, visualize the features in the data set and check for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from IPython.display import display\n",
    "\n",
    "from data.get_50_highest_weights import get_sp_50_highest_weights_symbols\n",
    "from data_preparation.ochlva_data import OCHLVAData\n",
    "from utils.column_modifiers import keep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('nbAgg')\n",
    "from utils.visualizations import plot_true_and_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the S&P 500 (as `^GSPC`) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ochlva_data = OCHLVAData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load three other stocks of interest: The stock weighted the most, the medium weighted stock and the lowest weighted stock (out of the 50 downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = get_sp_50_highest_weights_symbols()\n",
    "\n",
    "# Select symbols with high, medium and low weights\n",
    "selected_symbols = (symbols.iloc[0], \n",
    "                    symbols.iloc[len(symbols)//2], \n",
    "                    symbols.iloc[-1])\n",
    "\n",
    "for s in selected_symbols:\n",
    "    ochlva_data.load_data(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describing_column = 'Adj. Close'\n",
    "columns = ['Samples', 'Start date', 'End date', 'NaNs', 'Mean', 'Max', 'Min', 'Std', 'Q1', 'Q2', 'Q3']\n",
    "describing_dict = {col: list() for col in columns}\n",
    "index = list()\n",
    "for key in ochlva_data.raw_data.keys():\n",
    "    index.append(key)\n",
    "    description = ochlva_data.raw_data[key].describe()\n",
    "    describing_dict['Samples'].append(int(description.loc['count', describing_column]))\n",
    "    describing_dict['Start date'].append(str(ochlva_data.raw_data[key].index[0])[:10])\n",
    "    describing_dict['End date'].append(str(ochlva_data.raw_data[key].index[-1])[:10])\n",
    "    describing_dict['NaNs'].append(ochlva_data.raw_data[key].isnull().sum().sum())\n",
    "    describing_dict['Mean'].append(description.loc['mean', describing_column])\n",
    "    describing_dict['Max'].append(description.loc['max', describing_column])\n",
    "    describing_dict['Min'].append(description.loc['min', describing_column])\n",
    "    describing_dict['Std'].append(description.loc['std', describing_column])\n",
    "    describing_dict['Q1'].append(description.loc['25%', describing_column])\n",
    "    describing_dict['Q2'].append(description.loc['50%', describing_column])\n",
    "    describing_dict['Q3'].append(description.loc['75%', describing_column])\n",
    "\n",
    "display(pd.DataFrame(describing_dict, index=index)[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we can observe that:\n",
    "\n",
    "1. The number of samples are almost the same, indicating that some trade days are missing for some of the stocks\n",
    "2. There are no NaNs or 0s present\n",
    "3. ^GSPC has values roughly one order of magnitude larger than the rest of the stocks\n",
    "\n",
    "Note that the mean and standard deviation are the *sample* mean and standard deviation. \n",
    "I.e. it does not represent the true mean and standard deviation, at least not if the process is non-stationary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulaizing features other than adjusted close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now investigate the ohlc data.\n",
    "We will investigate the volume for itself, as the information in the volume data is of quite different character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['Open', 'High', 'Low', 'Close']\n",
    "\n",
    "for key in ochlva_data.raw_data.keys():\n",
    "    ax = ochlva_data.raw_data[key].loc[:, columns].plot(alpha=0.7)\n",
    "    _ = ax.set_ylabel('USD')\n",
    "    ax.set_title(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we see that overall, the ochl values follow each other to a high degree.\n",
    "It is important to note that they are not exactly the same, and in fact, how the open, high and low aligns with the closing value can give important hints about the closing value the next day.\n",
    "The sudden drops are typical due to stock splits (which is accounted for in the adjusted close price)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to investigate the volume of the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = (ochlva_data.raw_data['^GSPC'].loc[:, 'Volume']/100).plot(alpha=0.7)\n",
    "_ = ochlva_data.raw_data['AAPL'].loc[:, 'Volume'].plot(alpha=0.7)\n",
    "_ = ochlva_data.raw_data['CMCSA'].loc[:, 'Volume'].plot(alpha=0.7)\n",
    "_ = ochlva_data.raw_data['GILD'].loc[:, 'Volume'].plot(alpha=0.7)\n",
    "ax.legend([f'^GSPC/100', f'AAPL', f'CMCSA', 'GILD'])\n",
    "_ = ax.set_ylabel('Volume')\n",
    "ax.set_title(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the volume does not follow the same trend as the ochl data, and has far less structure.\n",
    "Note that the `^GSPC` volume has been divided by $100$ to make it easier to visually compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the training, validation and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will only be interested in training using the adjusted close values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 'Adj. Close' column\n",
    "ochlva_data.transform(keep_columns, ['Adj. Close'], copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looping through the stocks\n",
    "for key in ochlva_data.transformed_data.keys():\n",
    "    x = ochlva_data.transformed_data[key]\n",
    "    \n",
    "    # Split the data\n",
    "    x_train_full, x_test = \\\n",
    "        train_test_split(x, shuffle=False, test_size=.2)\n",
    "    x_train, x_validate = \\\n",
    "        train_test_split(x_train_full, shuffle=False, test_size=.2)\n",
    "    \n",
    "    # Plot the train and test set\n",
    "    ax = x_train.plot()\n",
    "    _ = x_validate.plot(ax=ax)\n",
    "    _ = x_test.plot(ax=ax)\n",
    "    ax.legend([f'{key} Train', f'{key} Validation', f'{key} Test'])\n",
    "    ax.grid()\n",
    "    _ = ax.set_ylabel('USD')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the non-stationary hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check for non-stationarity for the set which visually apears the most stationary (i.e. the test set of the `CMCSA` stock).\n",
    "\n",
    "We will use the [Augmented Dickey-Fuller](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)\n",
    "test to test the probability that the time series contain a unit root and thereby is [trend-stationary](https://en.wikipedia.org/wiki/Trend_stationary), i.e. if the trend is removed, then the resulting data is stationary.\n",
    "If the time series is trend-stationary it is at least not stationary.\n",
    "\n",
    "The hypotheses are:\n",
    "\n",
    "* Null hypothesis: The series are non-stationary (i.e. there exist an time dependent trend)\n",
    "* Alternative hypothesis: The series is stationary (i.e. no trend exist)\n",
    "\n",
    "Let us set the rejection hypothesis threshold of the p-value to $5 %$, meaning that we reject the null hypothesis (the series are stationary) if the p-value is less than or equal to $0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ochlva_data.transformed_data['CMCSA']\n",
    "_, x_test = train_test_split(x, shuffle=False, test_size=.2)\n",
    "adf, p_value, _, _, critical_values, _ = adfuller(x_test.values.flatten())\n",
    "print(f'ADF Statistic: {adf:.3}')\n",
    "print(f'p-value: {p_value:.3}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'    {key}: {value:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $p > 0.05$ we can see that there is no evidence to reject the null hypothesis.\n",
    "In other words, the time series is probably non-stationary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
