{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KNN tuning number og neighbors\n",
    "\n",
    "In this notebook we tune the number of neigbors used for optimal prediction of the stock prices.\n",
    "We build on the findings in `3.1.1-knn_prediction_tuning_features.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "from data.get_50_highest_weights import get_sp_50_highest_weights_symbols\n",
    "from data_preparation.ochlva_data import OCHLVAData\n",
    "from utils.column_modifiers import target_generator\n",
    "from utils.column_modifiers import feature_generator\n",
    "from utils.column_modifiers import keep_columns\n",
    "from utils.scorers import normalized_root_mean_square_error\n",
    "from estimators.predictions import calculate_rolling_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('nbAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.visualizations import plot_scores\n",
    "from utils.visualizations import plot_true_and_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the S&P 500 (as `^GSPC`) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ochlva_data = OCHLVAData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load three other stocks: The stock weighted the most, the medium weighted stock and the lowest weighted stock (out of the 50 downloaded). \n",
    "We do this in order to get a better feeling of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = get_sp_50_highest_weights_symbols()\n",
    "\n",
    "# Select symbols with high, medium and low weights\n",
    "selected_symbols = (symbols.iloc[0], \n",
    "                    symbols.iloc[len(symbols)//2], \n",
    "                    symbols.iloc[-1])\n",
    "\n",
    "for s in selected_symbols:\n",
    "    ochlva_data.load_data(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will only be interested in training using the adjusted close values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 'Adj. Close' column\n",
    "ochlva_data.transform(keep_columns, ['Adj. Close'], copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the target values for the data.\n",
    "The target columns will be shifted 7, 14 and 28 days with respect to 'Adj. Close'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [7, 14, 28]\n",
    "ochlva_data.transform(target_generator, 'Adj. Close', days, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the features on the validation set\n",
    "\n",
    "In order not to leak information of the unseen data into the tuning we will tune the number of features on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_days = 160\n",
    "neighbors_list = [1, 2, 3, 4, 5, 10, 20]\n",
    "\n",
    "validation_scores = dict()\n",
    "train_scores = dict()\n",
    "\n",
    "for key in ochlva_data.transformed_data.keys():\n",
    "    print(f'Processing {key}')\n",
    "    # Extract the features and targets\n",
    "    x = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[:-len(days)]] \n",
    "    y = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[-len(days):]]\n",
    "       \n",
    "    # Append the stock to the scores\n",
    "    validation_scores[key] = dict()\n",
    "    train_scores[key] = dict()\n",
    "    \n",
    "    for n in neighbors_list:        \n",
    "        reg = neighbors.KNeighborsRegressor(n_neighbors=n)\n",
    "        \n",
    "        x_w_features = feature_generator(x, 'Adj. Close', feature_days, copy=True)\n",
    "        \n",
    "        x_train, _, y_train, y_test = \\\n",
    "            train_test_split(x_w_features, y, shuffle=False, test_size=.2)    \n",
    "        x_train_for_validate, x_validate, y_train_for_validate, y_validate = \\\n",
    "            train_test_split(x_train, y_train, shuffle=False, test_size=.2)\n",
    "\n",
    "        # Obtain the day of prediction\n",
    "        # I.e. for a column named x + 2 days, we would expect the two last rows\n",
    "        # to contain nan\n",
    "        prediction_days = y_test.isnull().sum()\n",
    "\n",
    "        # Calculate validation scores\n",
    "        y_pred, y_pred_train = \\\n",
    "            calculate_rolling_prediction(reg,\n",
    "                                         x_train_for_validate,\n",
    "                                         x_validate,\n",
    "                                         y_train_for_validate,\n",
    "                                         y_validate, \n",
    "                                         prediction_days,\n",
    "                                         training_prediction=True)\n",
    "        validation_scores[key][n] = normalized_root_mean_square_error(y_validate, y_pred)\n",
    "        \n",
    "        # The true value of the trainings is the same as y_validate shifted by one day\n",
    "        y_train_true = y_train.loc[y_pred_train.index, :] \n",
    "        train_scores[key][n] = normalized_root_mean_square_error(y_train_true, y_pred_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_scores(train_scores, validation_scores, x_label='Neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the training and validation error is decreasing as we increase the number of features.\n",
    "There seem to be no signs of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the unseen test set\n",
    "\n",
    "We will now test how well the model with the features generalizes on the unseen test set.\n",
    "This will also act as a sanity check in order to see that what we have found so far is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimal_n = 1\n",
    "\n",
    "for key in ochlva_data.transformed_data.keys():\n",
    "    \n",
    "    x = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[:-len(days)]] \n",
    "    y = ochlva_data.transformed_data[key].\\\n",
    "        loc[:, ochlva_data.transformed_data[key].columns[-len(days):]]\n",
    "        \n",
    "    x_w_features = feature_generator(x, 'Adj. Close', feature_days, copy=True)   \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_w_features, y, shuffle=False, test_size=.2)\n",
    "    \n",
    "\n",
    "    # Obtain the day of prediction\n",
    "    # I.e. for a column named x + 2 days, we would expect the two last rows\n",
    "    # to contain nan\n",
    "    prediction_days = y_test.isnull().sum()\n",
    "    \n",
    "    # Make the regressor with the optimal n\n",
    "    reg = neighbors.KNeighborsRegressor(n_neighbors=optimal_n)\n",
    "\n",
    "    \n",
    "    # NOTE: We refit the model here with the same architecture as we used in the predictions above\n",
    "    #       However, the data will be different for each time as we do a rolling prediction\n",
    "    y_pred = calculate_rolling_prediction(reg,\n",
    "                                          x_train,\n",
    "                                          x_test,\n",
    "                                          y_train,\n",
    "                                          y_test, \n",
    "                                          prediction_days)\n",
    "    \n",
    "    # Plot the short and the long predictions seperately in order not to \n",
    "    # clutter the plot\n",
    "    _ = plot_true_and_prediction(x_test, y_pred, columns=['Adj. Close'], y_label='USD')\n",
    "    plt.show()\n",
    "    \n",
    "    # As the first prediction is on the training set, we subtract 1 in the \n",
    "    # indexing to account for this\n",
    "    # Calculate the normalized root mean squared error\n",
    "    nrmse = normalized_root_mean_square_error(y_test, y_pred)\n",
    "    \n",
    "    print((f'Normalized root mean squared error (averaged for the three '\n",
    "           f'predictions): {nrmse}'))\n",
    "    \n",
    "    print('-'*80)\n",
    "    print('\\n'*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with rolling prediction (notice overfitting if we do not do a rolling prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#optimal_features = 160\n",
    "\n",
    "#for key in ochlva_data.transformed_data.keys():\n",
    "    \n",
    "#    x = ochlva_data.transformed_data[key].\\\n",
    "#        loc[:, ochlva_data.transformed_data[key].columns[:-len(days)]] \n",
    "#    y = ochlva_data.transformed_data[key].\\\n",
    "#        loc[:, ochlva_data.transformed_data[key].columns[-len(days):]]\n",
    "        \n",
    "#    x_w_features = feature_generator(x, 'Adj. Close', optimal_features, copy=True)   \n",
    "    \n",
    "#    x_train, x_test, y_train, y_test = train_test_split(x_w_features, y, shuffle=False, test_size=.2)\n",
    "    \n",
    "\n",
    "    # Obtain the day of prediction\n",
    "    # I.e. for a column named x + 2 days, we would expect the two last rows\n",
    "    # to contain nan\n",
    "#    prediction_days = y_test.isnull().sum()\n",
    "    \n",
    "    # NOTE: We refit the model here with the same architecture and training data as we used in the rolling prediction\n",
    "    #       This is beacuse the rolling prediction fits and predict target by target\n",
    "#    y_pred = calculate_normal_prediction(reg,\n",
    "#                                         x_train,\n",
    "#                                         x_test,\n",
    "#                                         y_train,\n",
    "#                                         y_test, \n",
    "#                                         prediction_days)\n",
    "    \n",
    "    # Plot the short and the long predictions seperately in order not to \n",
    "    # clutter the plot\n",
    "#    _ = plot_true_and_prediction(x_test, y_pred, columns=['Adj. Close'], y_label='USD')\n",
    "#    plt.show()\n",
    "    \n",
    "    # As the first prediction is on the training set, we subtract 1 in the \n",
    "    # indexing to account for this\n",
    "    # Calculate the normalized root mean squared error\n",
    "#    nrmse = normalized_root_mean_square_error(y_test, y_pred)\n",
    "    \n",
    "#    print((f'Normalized root mean squared error (averaged for the three '\n",
    "#           f'predictions): {nrmse}'))\n",
    "    \n",
    "#    print('-'*80)\n",
    "#    print('\\n'*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
